{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3X3yg8s5c4GC"
   },
   "source": [
    "<div id=\"top\"></div> \n",
    "# Table of contents\n",
    "* <a href='#Submission-instructions'>Submission instructions</a>\n",
    "* <a href=\"#A-short-introduction-to-LaTeX\">A short introduction to LaTeX</a>\n",
    "* <a href=\"#Some-useful-numpy-functions\">Some useful numpy functions</a>\n",
    "* <a href=\"#The-start-of-this-homework\">The start of this homework</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EenkzVIELWuR"
   },
   "source": [
    "# HW 1 \n",
    "## By: Mansi Sakarvadia, Nathaniel (Nate) Fulmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_acIvMXMc4GD"
   },
   "source": [
    "# Submission instructions\n",
    "See announcement on Sakai for instructions. Do NOT email the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4o4ooC05c4GE"
   },
   "source": [
    "<a href=\"#top\">top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PLXzIWv0c4GF"
   },
   "source": [
    "# A short introduction to LaTeX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c6UlPOUmc4GF"
   },
   "source": [
    "## Introduction\n",
    "$ \\LaTeX $ is a markup language to typeset documents. You can use it to express math compactly and make the layout of your documents beautiful. For the purpose of this class, we focus on the first piece, that is writing mathematical formulations. Jupyter implements a subset of $\\LaTeX$. Therefore, you can use $ \\LaTeX $ to answers the problems in the assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gDAFF8lyc4GG"
   },
   "source": [
    "## Two modes in latex that can typeset the formulations\n",
    "1. Inline mode, start with an \\$, end with an \\$ (\\$...\\$). E.g. $a + b = \\frac{1}{3}$\n",
    "2. Display mode, start with two \\$\\$, end with two \\$\\$ (\\$\\$...\\$\\$). E.g. $$a+b=\\frac{1}{3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PH4kXFjkc4GH"
   },
   "source": [
    "## Basic Maths\n",
    "1. It is straight-forward for basic arithmetic operations. E.g. $+, - * /, a^b, a_b$.\n",
    "2. $ \\LaTeX$ already defined many useful symbols, macros, (or functions) to typeset the formulations. They start with \\, such as \\LaTeX is for the latex symbol. In some typeset functions you can give them parameters. E.g. \\frac{1}{3} typesets one over three, where the thing within the first {} is nominator, and the thing within the second {} is denominator. {} also helps group thing within it together. Consider the difference between a\\_b+1 ($a_b+1$) and a\\_{b+1} ($a_{b+1}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bWoZuhUdc4GI"
   },
   "source": [
    "## Useful symbols and functions\n",
    "1. fraction, \\frac{1}{3}. ($\\frac{1}{3}$)\n",
    "2. partial derivative, \\partial. ($\\partial$), Combined with 1), we have \\frac{\\partial f}{\\partial x}. ($\\frac{\\partial f}{\\partial x}$)\n",
    "3. summation, \\sum\\_{i=1}^{N}. ($\\sum_{i=1}^{N}$)\n",
    "4. products, \\prod\\_{i=1}^{N}. ($\\prod_{i=1}^{N}$)\n",
    "5. indexing, w\\_{i, j}. ($w_{i, j}$)\n",
    "6. frequently used Greek letters \\alpha, \\beta, \\gamma. ($\\alpha, \\beta, \\gamma$)\n",
    "7. gradient notation \\nabla. ($\\nabla$)\n",
    "8. vector forms $\\text{\\\\begin{bmatrix} x_{i, 1} \\\\ \\vdots \\\\ x_{i, p+1}  \\\\end{bmatrix}}$. ($\\begin{bmatrix} x_{i, 1} \\\\ \\vdots \\\\ x_{i, p+1} \\end{bmatrix}$) <br \\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "22Weg71uc4GI"
   },
   "source": [
    "<a href=\"#top\">top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rJcmTtXFc4GJ"
   },
   "source": [
    "# Some useful numpy functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QXbfJ3Vhc4GK"
   },
   "source": [
    "People usualy import numpy as the follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QBCxNDB0c4GL"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eRUq9DTuc4GP"
   },
   "source": [
    "Here we introduce some useful numpy functions that you might use in this homework.\n",
    "* zeros <br \\>\n",
    "zeros generates a multi-dimensional array that contains all zeros. <br \\>\n",
    "    * Example: <br \\>\n",
    "    ** zeros( (3, 2) ) ** generates a 2d array of size 3\\*2 (three rows and 2 columns) that contains zeros in all entries. <br \\>\n",
    "    ** zeros( (3, 2, 4) )** generates a 3d array of size 3\\*2\\*4 that also contains zero in all entries.\n",
    "* ones <br \\>\n",
    "Similar to zeros, ones generates a multi-dimensional array that contains all ones. <br \\>\n",
    "* exp <br \\>\n",
    "exp takes exponential on each entry of the input. <br \\>\n",
    "    * Example: <br \\>\n",
    "    ** exp(3) ** computes $ e^3 $. <br \\>\n",
    "    ** exp( [1, 2, 3] ) ** computes $[ e^1, e^2, e^3]$.\n",
    "* log <br \\>\n",
    "Similar to exp, log takes log on each entry of the input. Since log function is not defined on the values $<= 0$, if the input of log is like that, it will output nan or inf defined in numpy. <br \\>\n",
    "* sum <br \\>\n",
    "sum takes sum over an axis of the input array. <br \\>\n",
    "    * Example: please see the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "IT4E98_bc4GQ",
    "outputId": "47d1363c-9336-4a85-d656-29fe0e3b4ceb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input array is a 3*3 matrix, values: \n",
      " [[0 1 2]\n",
      " [3 4 5]\n",
      " [6 7 8]]\n",
      "result 1 is  [ 9 12 15]\n",
      "result 2 is  [ 3 12 21]\n"
     ]
    }
   ],
   "source": [
    "matrixA = np.arange(9).reshape(3, 3)\n",
    "print( 'input array is a 3*3 matrix, values: \\n', matrixA )\n",
    "result1 = np.sum(matrixA, axis = 0)\n",
    "result2 = np.sum(matrixA, axis = 1)\n",
    "print( 'result 1 is ', result1 )\n",
    "print( 'result 2 is ', result2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7SRzIIUHc4GV"
   },
   "source": [
    "* dot <br \\>\n",
    "do dot-product of the two inputs (if they are vectors), or do matrix multiplication of the two inputs (if they are 2d arrays). In numpy if A and B are both arrays, A\\*B computes the element-wise multiplication, not matrix multiplication. To use dot, the dimensions of the two inputs must be valid. <br \\>\n",
    "    * Example: please see the following code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "pxUydb5Bc4GW",
    "outputId": "fa0c7da9-27c7-44e8-f4ee-546d1d60560a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vecotr 1 is a length-3 vector, values:  [1 2 3]\n",
      "vecotr 2 is a length-3 vector, values:  [1 2 1]\n",
      "dot product of vecotr 1 and 2 is 8\n",
      "dot product of vecotr 1 and 3 generates error\n"
     ]
    }
   ],
   "source": [
    "#dot product examples\n",
    "vector1 = np.array( [1, 2, 3] )\n",
    "vector2 = np.array( [1, 2, 1] )\n",
    "print( 'vecotr 1 is a length-3 vector, values: ', vector1 )\n",
    "print( 'vecotr 2 is a length-3 vector, values: ', vector2 )\n",
    "print( 'dot product of vecotr 1 and 2 is', np.dot(vector1, vector2) )\n",
    "vector3 = np.array( [1, 2, 1, 2])\n",
    "print( 'dot product of vecotr 1 and 3 generates error' )\n",
    "#np.dot(vector1, vector3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "colab_type": "code",
    "id": "iP72ULqzc4Ga",
    "outputId": "527f0c43-f279-40ac-8624-649dc359920a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix1 is a 2*3 matrix, values: \n",
      "  [[0 1 2]\n",
      " [3 4 5]]\n",
      "matrix2 is a 3*1 matrix, values: \n",
      "  [[1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "matrix1 multiplies matrix2 is a 2*1 matrix: \n",
      "  [[ 3.]\n",
      " [12.]]\n"
     ]
    }
   ],
   "source": [
    "#matrix multiplication examples\n",
    "matrix1 = np.arange(6).reshape(2, 3)\n",
    "matrix2 = np.ones(((3,1)))\n",
    "print( 'matrix1 is a 2*3 matrix, values: \\n ', matrix1 )\n",
    "print( 'matrix2 is a 3*1 matrix, values: \\n ', matrix2 )\n",
    "print( 'matrix1 multiplies matrix2 is a 2*1 matrix: \\n ', np.dot(matrix1, matrix2) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BczRxYTvc4Ge"
   },
   "source": [
    "* T <br \\>\n",
    "[numpy matrix variable].T, takes transpose of the input matrix variable.\n",
    "    * Example: please see the following code cell <br \\>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "ydSWj9Fdc4Gh",
    "outputId": "6446d2e0-dda4-47fd-97f1-66a65363ae2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix dimension is (4, 3)\n",
      "matrix dimension after transpose is (3, 4)\n"
     ]
    }
   ],
   "source": [
    "matrix1 = np.ones( (4, 3) )\n",
    "print( 'matrix dimension is',  matrix1.shape )\n",
    "print( 'matrix dimension after transpose is', matrix1.T.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nmaLewJoc4Gn"
   },
   "source": [
    "<a href=\"#top\">top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_elh2tjPc4Go"
   },
   "source": [
    "# The start of this homework\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rDYjxzIXc4Go"
   },
   "source": [
    "# Import plotting utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aBz0EUeLc4Gp"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "95W5XcbLc4Gs"
   },
   "source": [
    "# Download and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "UjfilLNVc4Gs",
    "outputId": "52106e89-3189-4105-c09d-9bdc5e7a6824"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should take at most 30 seconds\n",
      "Time elapsed (seconds): 0.0019910335540771484\n"
     ]
    }
   ],
   "source": [
    "#download data\n",
    "import time\n",
    "import urllib\n",
    "import os.path\n",
    "import sys\n",
    "versionName = sys.version_info\n",
    "if versionName[0] == 2:\n",
    "    import urllib as U\n",
    "elif versionName[0] == 3:\n",
    "    import urllib.request as U\n",
    "start = time.time()\n",
    "print(\"Should take at most 30 seconds\")\n",
    "if not os.path.isfile('train_data.pgz'):\n",
    "    U.urlretrieve(\"https://sakai.unc.edu/access/content/group/c4f84923-328b-429b-a8dc-a340b0284e41/HW1/train_data.pgz\", \"train_data.pgz\")\n",
    "if not os.path.isfile('test_data.pgz'):\n",
    "    U.urlretrieve(\"https://sakai.unc.edu/access/content/group/c4f84923-328b-429b-a8dc-a340b0284e41/HW1/test_data.pgz\", \"test_data.pgz\")\n",
    "if not os.path.isfile('vocab_list.pgz'):\n",
    "    U.urlretrieve( \"https://sakai.unc.edu/access/content/group/c4f84923-328b-429b-a8dc-a340b0284e41/HW1/vocab_list.pgz\", \"vocab_list.pgz\" );\n",
    "end = time.time()\n",
    "print(\"Time elapsed (seconds):\", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Wz1StQ3rc4Gv",
    "outputId": "067135c3-0a1e-43eb-9293-aa0333f48713"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should take about 15 seconds\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "    kwargs = {}\n",
    "except:\n",
    "    import _pickle as pickle\n",
    "    kwargs = {'encoding':'bytes'}\n",
    "    \n",
    "import gzip\n",
    "import numpy as np\n",
    "start = time.time()\n",
    "print(\"Should take about 15 seconds\")\n",
    "train_data, train_label = pickle.load( gzip.open( \"train_data.pgz\", \"rb\" ), **kwargs )\n",
    "train_label = np.asarray(train_label)\n",
    "test_data = pickle.load( gzip.open( \"test_data.pgz\", \"rb\" ),**kwargs )\n",
    "vocab_list = pickle.load( gzip.open( \"vocab_list.pgz\", \"rb\" ),**kwargs )\n",
    "end = time.time()\n",
    "print(\"Time elapsed (seconds):\", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "e41K3e3Yc4G3",
    "outputId": "6b7a4e75-df7d-4b52-e379-b328a27962a6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Consider using small set of the data\n",
    "trainData = train_data[:10000, :]\n",
    "validData = train_data[10000:15000, :]\n",
    "trainLabel = train_label[:10000]\n",
    "validLabel = train_label[10000:15000]\n",
    "testData = test_data[:10000, :]\n",
    "print( vocab_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ko1xLwdLc4G6",
    "outputId": "9d33b8c4-ea74-4167-c74a-b7acbcc45403"
   },
   "outputs": [],
   "source": [
    "train_data.mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z47N4iv7c4HA"
   },
   "source": [
    "<div id=\"top_of_steps\"></div>\n",
    "# Steps\n",
    "1. <a href=\"#Implement-logistic-regression-likelihood.\">Implement logistic regression likelihood.</a>\n",
    "2. <a href=\"#Compute-derivative-of-logistic-regression.\">Compute derivative of logistic regression.</a>\n",
    "3. <a href=\"#Check-gradient.\">Check gradient.</a>\n",
    "4. <a href=\"#Tweak-gradient-ascent-code.\">Tweak gradient ascent code.</a>\n",
    "5. <a href=\"#Report-results-and-analysis.\">Report results and analysis.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pvkaqh1yc4HA"
   },
   "source": [
    "# Implement logistic regression likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAseMQMMc4HB"
   },
   "source": [
    "Data is given as $D = {(\\mathbf{x}_i, y_i):, i = 1...n}$, where $y_i \\in \\{-1, +1\\}$, and $\\mathbf{x}_i \\in R^p$. In this case there are n samples and each sample has p features. <br \\>\n",
    "\n",
    "For logistic regression, \n",
    "* We have model parameters: $\\mathbf{w} \\in R^p$ for weight and a bias term $b$.\n",
    "* For a sample x and its label y, $p(y|\\mathbf{x}, \\mathbf{w}, b) = \\frac{1}{1+exp\\{-y(\\mathbf{w} \\cdot \\mathbf{x} + b)\\}}$ \n",
    "* We can define $x' = \\begin{bmatrix} 1\\\\ x \\end{bmatrix}$, then $ \\mathbf{w}' =  \\begin{bmatrix} b\\\\ \\mathbf{w} \\end{bmatrix}$. Therefore the bias term is included in the weight vector. For notation brevity, we still use notations $x, \\mathbf{w}$ as $x', \\mathbf{w}'$. This can be implemented by numpy.concatenate function.\n",
    "* Hence the first entry of the vector $w'$ is bias term and the rest are feature weights. In the code you can use w[0] to access the bias term and w[1:] to access the feature weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HNZZjAqic4HB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#We help you do the concatenate, so the first feature becomes the  bias term\n",
    "train_data_pad = np.concatenate( ( np.ones((trainData.shape[0], 1)), trainData ), axis = 1 )\n",
    "test_data_pad = np.concatenate( ( np.ones((testData.shape[0], 1)), testData ), axis = 1 )\n",
    "valid_data_pad = np.concatenate( ( np.ones((validData.shape[0], 1)), validData ), axis = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9lo1x2z7c4HG"
   },
   "source": [
    "## To-do: \n",
    "1. Given the data $D = {(\\mathbf{x}_i, y_i)} $, write down the likelihood function ($L(\\mathbf{w})$) of logistic regression. ** [1 pt] **\n",
    "2. Take $\\log$ of the likelihood function in (1), write down the log likelihood function. Hint: $\\log$ will not cancel $\\exp$. ** [1 pt] **\n",
    "3. Add  ridge penalty in the log likelihood function (Let the weight of ridge penalty be $\\alpha$). Hint: Do not include $w_0$ in the ridge term. ** [1 pt] **\n",
    "4. Write a function to compute regularized log likelihood ** [1 pt] **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5WEogX95c4HG"
   },
   "source": [
    "1. $ L(\\mathbf{w'}) =  \\displaystyle\\prod_{i}\\frac{1}{1+exp\\{-y_i(\\mathbf{w'} \\cdot \\mathbf{x_{i}'} )\\}}$ \n",
    "2. $ LL(\\mathbf{w'}) = -\\sum_i^{N} log(1+exp(-y_i(\\mathbf{w'} \\cdot \\mathbf{x_{i}'} )))$\n",
    "3. $ PLL(\\mathbf{w'}) = -\\sum_{i}^{N} log(1+exp(-y_i(\\mathbf{w'} \\cdot \\mathbf{x_i'}))  - \\frac{\\alpha}{2}\\sum_{j=1}^{p} \\mathbf{w_{j}'}^2 $\n",
    "\n",
    "##Notes: do we need to adjust the order of y,w,x? + 9/7: added i subscript to w based on lect05 logistic regression--log-likelihood + added N b/c lect05 Bayesian view of penalties section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cAFbB3cKc4HH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def loglikelihood(w, X, y, alpha): \n",
    "    #compute loglikelihood for current w, b, given the data X, y\n",
    "    #w is a vector, b is a scalr, X is a n*p matrix and y is a vector.\n",
    "    arrn1 = np.dot(X,w) #arrn1 is nx1 array\n",
    "    temparr = np.ones(len(y)).transpose()  #arry of ones columns nx1\n",
    "    np.multiply(-y, arrn1, temparr)\n",
    "   # for i in range(len(y)):\n",
    "   #   temparr[i] = -y[i]*arrn1[i]\n",
    "    tmp = np.log(1. + np.exp(temparr))\n",
    "    return -np.sum( tmp ) - (alpha/2.)*np.sum( np.dot(w[1:], w[1:]) ) #indexing starts at 1 bc we watnt to avoid including B0 in penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "8UPjN3pfc4HP",
    "outputId": "6b3f420a-7edd-4d9f-c143-5ba657b5243e"
   },
   "outputs": [],
   "source": [
    "# testing\n",
    "np.random.seed(1)\n",
    "X = np.random.randn(2,3)\n",
    "y = np.array([1,-1])\n",
    "w = np.ones(3)\n",
    "w[[1]] = -1;\n",
    "loglikelihood(w, X, y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "0mimAN24c4HT",
    "outputId": "1dc04e31-5cb0-4e68-9922-fec812f86626"
   },
   "outputs": [],
   "source": [
    "#the values printed in this cell should be the same as the value printed in the previous cell.\n",
    "print( -np.log(1+np.exp(-1*(X[0,0]-X[0,1]+X[0,2]))) - np.log(1+np.exp(1*(X[1,0]-X[1,1]+X[1,2]))) -1/2.*np.sum(w[1:]**2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_8dvoKvEc4HW"
   },
   "source": [
    "<a href=\"#top_of_steps\">top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-XcaXVRc4HX"
   },
   "source": [
    "# Compute derivative of logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIt1TUq5c4HX"
   },
   "source": [
    "In order to optimize the function, we want to take the derivative of the function, and update $\\mathbf{w}$ according to the direction of the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "twYXd16Ic4HY"
   },
   "source": [
    "## To-do:\n",
    "1. Write down the derivative of the **penalized log likelihood function** for each $ w_j $. Hint: Remember that bias term is $w_0$ and treat it separately from the rest of $w_j$, $j\\in\\{1,...,p\\}$ ** [1 pt] **\n",
    "2. Write down the gradient of log likelihood function. Hint: You can express this in terms of probabilities. ** [1 pt] **\n",
    "3. Update the loglikelihood function to return both the loglikelihood and the gradient. ** [1 pt] **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gyB60z_5c4HY"
   },
   "source": [
    "1. \n",
    "$ \\frac{\\partial PLL(\\mathbf{w'})}{ \\partial \\mathbf{w_0}} = \\sum_i^N \\frac{y_iexp{(-y_i\\mathbf{w'x_i})}}{1+exp{(-y_i\\mathbf{w'x_i'})}}$  \n",
    "\n",
    "\n",
    "> \n",
    "\n",
    "$ \\frac{\\partial PLL(\\mathbf{w'})}{ \\partial \\mathbf{w_j}} = \\sum_i^N \\frac{y_i\\mathbf{x_j'}exp{(-y_i\\mathbf{w'x_i'})}}{1+exp{(-y_i\\mathbf{w'x_i'})}} $ \n",
    "$  - \\alpha\\mathbf{w_j'}, j>0$ \n",
    ">\n",
    "2. <br \\>\n",
    "$ \\nabla PLL(\\mathbf{w}) = \\sum_i^N y_i\\begin{bmatrix}  \\\\\\frac{exp{(-y_i\\mathbf{w'x_i})}}{1+exp{(-y_i\\mathbf{w'x_i'})}} \\\\\\frac{\\mathbf{x_j'}exp{(-y_i\\mathbf{w'x_i'})}}{1+exp{(-y_i\\mathbf{w'x_i'})}} \\\\ \\vdots \\\\ \\frac{\\mathbf{x_p'}exp{(-y_i\\mathbf{w'x_i'})}}{1+exp{(-y_i\\mathbf{w'x_i'})}} \\end{bmatrix} - \\alpha\\begin{bmatrix}  \\\\ 0 \\\\ w_{j} \\\\ \\vdots \\\\ w_{p}\\\\ \\end{bmatrix},  j>0 $\n",
    "\n",
    "##NOTE: Summation for weight on part 1, how to index j...p in gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EWF3LKDRRLMk"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def loglikelihood(w, X, y, alpha): \n",
    "    #compute loglikelihood for current w, b, given the data X, y\n",
    "    #w is a vector, b is a scalr, X is a n*p matrix and y is a vector.\n",
    "    #########\n",
    "    xw = np.dot(X,w) #xw is nx1 array\n",
    "    neg_yxw = np.ones(len(y)).transpose()  #arry of ones columns nx1\n",
    "    for i in range(len(y)):\n",
    "      neg_yxw[i] = -y[i]*xw[i]\n",
    "    tmp = 1. + np.exp(neg_yxw, dtype = np.longdouble)\n",
    "    penalty = (alpha/2.)*np.sum( np.dot(w[1:], w[1:]), dtype = np.longdouble )\n",
    "    ##########\n",
    "   \n",
    "    X = X.T #X becomes a p*n matrix so the gradVal can be compute straight-forwardly.\n",
    "    gradVal = np.ones(len(w)).transpose() # empty gradient\n",
    "    gradPenalty = np.ones(len(w)).transpose() # empty gradPenalty (bias term has no penalty)\n",
    "\n",
    "    numerator = np.ones(len(y)).transpose() # empty vector for numerator of summations\n",
    "    #Gradient for rest of weights:\n",
    "    for (i) in range(len(w)-1):\n",
    "      np.multiply(y, X[i+1],numerator, dtype = np.longdouble)\n",
    "      np.multiply(numerator, np.exp(neg_yxw), numerator, dtype = np.longdouble)\n",
    "      np.divide(numerator, tmp, numerator, dtype = np.longdouble)\n",
    "      gradVal[i+1] = np.sum(numerator, dtype = np.longdouble) # sum((yxexp(-ywx'))/(1+exp(-ywx')))   \n",
    "      gradPenalty[i+1] = -alpha*w[i+1]     \n",
    "\n",
    "    gradPenalty[0] = 0 #(bias term has no penalty) \n",
    "\n",
    "    #The gradient for the bias term:\n",
    "    np.multiply(y, np.exp(neg_yxw),numerator, dtype = np.longdouble)\n",
    "    np.divide(numerator, tmp, numerator, dtype = np.longdouble)\n",
    "    gradVal[0] = np.sum(numerator, dtype = np.longdouble) #sum((y*exp(-ywx'))/(1+exp(-ywx'))); BIAS TERM GRAD\n",
    "\n",
    "    return -np.sum( np.log( tmp , dtype = np.longdouble), dtype = np.longdouble ) - penalty, gradVal + gradPenalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tZRO9WJfc4Hb"
   },
   "source": [
    "<a href=\"#top_of_steps\">top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QHd-ZruKc4Hc"
   },
   "source": [
    "# Check gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gw8l5AJOc4Hc"
   },
   "source": [
    "It is very important we know the derivative we computed is correctly. We can check it by comparing it with numerical answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BaNL_y94c4Hd"
   },
   "outputs": [],
   "source": [
    "# %load grad_check.py\n",
    "def grad_check(f,xy0,delta=1e-6,tolerance=1e-7):\n",
    "    f0,g0 = f(xy0)\n",
    "    p = len(xy0)\n",
    "    finite_diff = np.zeros(p)\n",
    "    gradient_correct = True\n",
    "    for i in range(p):\n",
    "        xy1 = np.copy(xy0)\n",
    "        xy2 = np.copy(xy0)\n",
    "        xy1[i] = xy1[i] - 0.5*delta\n",
    "        xy2[i] = xy2[i] + 0.5*delta\n",
    "        f1,_ = f(xy1)\n",
    "        f2,_ = f(xy2)\n",
    "        finite_diff = (f2 - f1)/(delta)\n",
    "        if (abs(finite_diff - g0[i])>tolerance):\n",
    "            print(\"Broken partial\",i,\" Finite Diff: \",finite_diff,\" Partial: \",g0[i])\n",
    "            gradient_correct = False\n",
    "    return gradient_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3eDiv4iQc4Hf"
   },
   "source": [
    "* We initialize the w vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "akPbktfic4Hg"
   },
   "outputs": [],
   "source": [
    "w_init = np.random.randn( train_data_pad.shape[1] )*0.001\n",
    "w_init[0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WLOdTqIRc4Hj"
   },
   "source": [
    "## To-do:\n",
    "* Here is the code to test if your gradient computation is correct (If the result is true, you get **1 pt**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "BRb2bSezc4Hk",
    "outputId": "b75c882a-574d-4b26-b53c-758968e2709a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = lambda xy0: loglikelihood(xy0, X=train_data_pad[:,:15], y=trainLabel, alpha=1)\n",
    "grad_check( g, w_init[:15], delta=1e-6, tolerance=1e-5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DMnCExVwc4Ho"
   },
   "source": [
    "<a href=\"#top_of_steps\">top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9p6WjXdxc4Hp"
   },
   "source": [
    "# Tweak gradient ascent code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wwAiQBbcc4Hp"
   },
   "source": [
    "Here we provide the gradient ascent function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jolypiO2c4Hq"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# %load gradient_ascent\n",
    "def gradient_ascent(f,x,init_step,iterations):  \n",
    "    f_val,grad = f(x)                           # compute function value and gradient \n",
    "    f_vals = [f_val]\n",
    "    for it in range(iterations):                # iterate for a fixed number of iterations\n",
    "        #print 'iteration %d' % it\n",
    "        done = False                            # initial condition for done\n",
    "        line_search_it = 0                      # how many times we tried to shrink the step\n",
    "        step = init_step                        # reset step size to the initial size\n",
    "        while not done and line_search_it<100:  # are we done yet?\n",
    "            new_x = x + step*grad               # take a step along the gradient\n",
    "            new_f_val,new_grad = f(new_x)       # evaluate function value and gradient\n",
    "            if new_f_val<f_val:                 # did we go too far?\n",
    "                step = step*0.95                # if so, shrink the step-size\n",
    "                line_search_it += 1             # how many times did we shrank the step\n",
    "            else:\n",
    "                done = True                     # better than the last x, so we move on\n",
    "        \n",
    "        if not done:                            # did not find right step size\n",
    "            print(\"Line Search failed.\")\n",
    "        else:\n",
    "            f_val = new_f_val                   # ah, we are ok, accept the new x\n",
    "            x = new_x\n",
    "            grad = new_grad\n",
    "            f_vals.append(f_val)\n",
    "        plt.plot(f_vals)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Function value')\n",
    "    return f_val, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n1R4yCFnc4Ht"
   },
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "w_init = np.random.randn( train_data_pad.shape[1] )*0.001\n",
    "w_init[0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "69xaJ8W8c4Hv"
   },
   "source": [
    "## To-do:\n",
    "* Try different init_step (1e-4, 1e-5, 1e-6) using the following code, report the final regularized log-likelihood values. **[1 pt]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rGZmeIjnc4Hw"
   },
   "outputs": [],
   "source": [
    "def optimizeFn( init_step, iterations, alpha, w):\n",
    "    g = lambda xy0: loglikelihood(xy0, train_data_pad, trainLabel, alpha)\n",
    "    f_val, update_w = gradient_ascent( g, w, init_step, iterations )\n",
    "    return f_val, update_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "colab_type": "code",
    "id": "kBHPnQQAc4Hy",
    "outputId": "6d2a7c28-2dd4-4790-bcc3-742e48e86245",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print ('This should take about 6 seconds.')\n",
    "start = time.time()\n",
    "f_val, update_w=optimizeFn( init_step = 1e-6, iterations=100, alpha=0, w = w_init) #set init_step to 1e-4, 1e-5, 1e-6\n",
    "end = time.time()\n",
    "print ('Time elapsed (seconds):', end-start)\n",
    "print ('final log-likelihood = %f\\n' % (f_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gRG9UOXdc4H2"
   },
   "source": [
    "Final regularized log-likelihood values for (1e-4, 1e-5, 1e-6) are: \n",
    ">\n",
    " - 1e-4: -2602.170368\n",
    " - 1e-5: -3033.038249\n",
    " - 1e-6: -4707.155301"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CG01sWXac4H3"
   },
   "source": [
    "<a href=\"#top_of_steps\">top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AI3h8GHmc4H4"
   },
   "source": [
    "# Report results and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZpUS3UXSc4H4"
   },
   "source": [
    "To evaluate the results, we need to have a prediction function, that uses the model we trained to predict the comment is positive and negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YMp2HWFfc4H5"
   },
   "source": [
    "## To-do:\n",
    "1. Implement the prediction function. It should take as inputs feature weights and feature matrix. It should return vector of labels. **[1 pt]**\n",
    "2. Try different alpha (1000, 2000, 3000), and report which alpha produces the model that has the best accuracy on the validation set **[1 pt]**\n",
    "2. **[optional]** Report one sample that is classified wrong with high probabilites (> 90%). **[1 pt]**\n",
    "3. **[optional]** Report the words (entries in vocab_list associated with that feature) that cause the sample reported in (2) classify wrong. Note that weight w[i] correponds to word vocab_list[i-1], because we included bias term in w.**[1 pt]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WXkgs8BQc4H6"
   },
   "outputs": [],
   "source": [
    "def prediction(w, validData ):\n",
    "    prob = 1./(1+ np.exp(-np.dot(validData,w)));    \n",
    "    res = np.zeros(validData.shape[0])\n",
    "    res[prob>=0.5] = 1\n",
    "    res[prob<0.5] = -1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "FRU6s8X4c4H8",
    "outputId": "98f3f8e1-f649-4e94-9a74-a715eaa6b0a5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#see the accuracy on the validation set\n",
    "#when init_step=1e-5, the model has the best accuracy in the validation set\n",
    "f_val, update_w=optimizeFn( init_step = 1e-5, iterations=100, alpha=3000, w=w_init) #try different alphas [1000, 2000, 3000]\n",
    "pred = prediction(update_w, valid_data_pad)\n",
    "print( 'accuracy on the validation set {:.2f}%'.format( 100.*np.mean(pred==validLabel)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xnS0HXEic4H_"
   },
   "source": [
    "The best alpha is 3000, and the accuracy of this alpha is: 84.74%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IW9icklhc4IA"
   },
   "source": [
    "* Report one sample (sample index in the validation data set) that is classified wrong with high probabilites\n",
    "\n",
    "##TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-iisdBzc4IA"
   },
   "outputs": [],
   "source": [
    "wrong_idx = np.nonzero( validLabel != pred )[0] #use this command to get the samples that are predicted wrong\n",
    "correct_idx = np.nonzero( validLabel == pred )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "jlBkT2jJtCJh",
    "outputId": "6087ecb6-5218-464e-c3a6-ad217d18a02c"
   },
   "outputs": [],
   "source": [
    "wrong_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A92KphTuc4ID"
   },
   "outputs": [],
   "source": [
    "#implement the function to compute probability\n",
    "def computeProb(w, validData ):\n",
    "    prob = 1./(1+ np.exp(-np.dot(validData,w))); \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "9yMvraA1c4IH",
    "outputId": "2525ad75-6baa-49c2-b2d3-2e3ab514759c"
   },
   "outputs": [],
   "source": [
    "#get the samples that are classified wrong and with probabilites > 0.9\n",
    "probs = computeProb(update_w, valid_data_pad)\n",
    "wrong_idx_high = []\n",
    "for i in range(len(probs)):\n",
    "  if i in wrong_idx and probs[i]>0.9:\n",
    "    wrong_idx_high.append(i)\n",
    "wrong_idx_high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m2LH4fLWc4IM"
   },
   "source": [
    "The sample index is 2027 (see above for more indexes with high probablities and wrong classifications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "11S-w2sdc4IM"
   },
   "source": [
    "* Report the words that cause the sample reported in (2) classify wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zwz2ZQzFc4IN"
   },
   "outputs": [],
   "source": [
    "#Use this function to get the most important words for each sample index\n",
    "#This function returns a list of top 10 words that influence the prediction.\n",
    "def getMostImportantFeatures( sampleIdx, validData, update_w, vocab_list ):\n",
    "    confusedList = []\n",
    "    intensity = validData[sampleIdx,:]*update_w\n",
    "    tmp = np.argsort( np.abs(intensity[:]) )[::-1]\n",
    "    for j in np.arange(10):\n",
    "        confusedList.append(vocab_list[tmp[j]-1])\n",
    "    return confusedList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "colab_type": "code",
    "id": "YcE-qU6Ic4IP",
    "outputId": "ab1409d0-412f-48d4-fc2a-080ae01d1f8e"
   },
   "outputs": [],
   "source": [
    "confusedList = getMostImportantFeatures(2027, valid_data_pad, update_w, vocab_list) #use the sample index got from the previous result\n",
    "confusedList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pOebErknc4IT"
   },
   "outputs": [],
   "source": [
    "#load file ids\n",
    "if not os.path.isfile('train_id.pgz'):\n",
    "    U.urlretrieve( \"https://sakai.unc.edu/access/content/group/c4f84923-328b-429b-a8dc-a340b0284e41/HW1/train_id.pgz\", \"train_id.pgz\" );\n",
    "train_id = pickle.load( gzip.open( \"train_id.pgz\", \"rb\" ) )\n",
    "valid_id = train_id[10000:15000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-PAMjukyc4Ib"
   },
   "source": [
    "* Retrieve the whole review and check if it is hard to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GNMakOAQc4Ib"
   },
   "outputs": [],
   "source": [
    "fileName = valid_id[100]\n",
    "fileUrl = \"https://wwwx.cs.unc.edu/Courses/comp755-f18/hw1/reviews/\" + fileName + '.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "zRR50Vpmc4Ig",
    "outputId": "3ff716ed-5183-49c3-d5be-cbed5cfea56e"
   },
   "outputs": [],
   "source": [
    "U.urlretrieve(fileUrl, fileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "RCm2a0Lec4Ik"
   },
   "source": [
    "<a href=\"#top_of_steps\">top</a>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "hw1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
